# üìò Guide de D√©ploiement sur Google Cloud Platform

Ce guide vous accompagne pas √† pas pour d√©ployer l'automatisation compl√®te sur GCP.

## üéØ Objectif final

Avoir une extraction automatique quotidienne qui :
- ‚úÖ S'ex√©cute tous les jours √† 2h du matin
- ‚úÖ Stocke les donn√©es dans BigQuery
- ‚úÖ G√®re les erreurs et les rattrapages automatiques
- ‚úÖ Envoie des rapports hebdomadaires

## üìã Pr√©requis

1. **Compte Google Cloud** avec facturation activ√©e
2. **Cl√© API SimilarWeb** valide
3. **gcloud CLI** install√© ([guide d'installation](https://cloud.google.com/sdk/docs/install))
4. **Droits administrateur** sur le projet GCP

## üöÄ √âtape 1 : Configuration initiale GCP

### 1.1 Cr√©er un nouveau projet
```bash
# Se connecter √† GCP
gcloud auth login

# Cr√©er un nouveau projet
gcloud projects create similarweb-data-[votre-nom] --name="SimilarWeb Data Pipeline"

# D√©finir ce projet comme projet par d√©faut
gcloud config set project similarweb-data-[votre-nom]
```

### 1.2 Activer les APIs n√©cessaires
```bash
# Activer les services requis
gcloud services enable cloudfunctions.googleapis.com
gcloud services enable cloudscheduler.googleapis.com
gcloud services enable bigquery.googleapis.com
gcloud services enable cloudbuild.googleapis.com
gcloud services enable storage.googleapis.com
```

### 1.3 Configurer la facturation
```bash
# Lister vos comptes de facturation
gcloud billing accounts list

# Lier le compte de facturation au projet
gcloud billing projects link similarweb-data-[votre-nom] \
  --billing-account=[BILLING_ACCOUNT_ID]
```

## üóÑÔ∏è √âtape 2 : Configuration BigQuery

### 2.1 Cr√©er le dataset
```bash
# Cr√©er le dataset pour stocker les donn√©es
bq mk --dataset \
  --location=EU \
  --description="Donn√©es SimilarWeb - Segments et Sites Web" \
  similarweb-data-[votre-nom]:similarweb_data
```

### 2.2 Cr√©er les tables
```bash
# Se placer dans le dossier des scripts GCP
cd gcp_deployment/scripts

# Rendre le script ex√©cutable
chmod +x create_bigquery_tables.sh

# Modifier le PROJECT_ID dans le script
sed -i '' 's/similarweb-intel-dev/similarweb-data-[votre-nom]/g' create_bigquery_tables.sh

# Ex√©cuter le script
./create_bigquery_tables.sh
```

### 2.3 V√©rifier les tables
```bash
# Lister les tables cr√©√©es
bq ls similarweb_data

# Voir le sch√©ma d'une table
bq show similarweb_data.segments_data
```

## ‚öôÔ∏è √âtape 3 : Pr√©parer la Cloud Function

### 3.1 Configurer les variables d'environnement
```bash
cd gcp_deployment/cloud_functions

# Cr√©er le fichier .env.yaml pour la Cloud Function
cat > .env.yaml << EOF
SIMILARWEB_API_KEY: "VOTRE_CLE_API_ICI"
GCP_PROJECT_ID: "similarweb-data-[votre-nom]"
BIGQUERY_DATASET: "similarweb_data"
USER_ONLY_SEGMENTS: "true"
EOF
```

### 3.2 Mettre √† jour la configuration
```python
# √âditer config.py pour mettre √† jour le PROJECT_ID
# Remplacer 'similarweb-intel-dev' par 'similarweb-data-[votre-nom]'
```

### 3.3 Cr√©er le bucket de stockage
```bash
# Cr√©er un bucket pour les logs et backups
gsutil mb -l EU gs://similarweb-data-[votre-nom]-storage
```

## üöÄ √âtape 4 : D√©ployer la Cloud Function

### 4.1 D√©ploiement
```bash
# Se placer dans le bon dossier
cd gcp_deployment/scripts

# Modifier le script de d√©ploiement
sed -i '' 's/similarweb-intel-dev/similarweb-data-[votre-nom]/g' deploy_cloud_function.sh

# D√©ployer la fonction
./deploy_cloud_function.sh
```

### 4.2 Tester la fonction
```bash
# Tester avec un appel manuel
gcloud functions call similarweb-daily-extraction \
  --region=europe-west1 \
  --data='{"test_mode": true}'

# Voir les logs
gcloud functions logs read similarweb-daily-extraction \
  --region=europe-west1 \
  --limit=50
```

## ‚è∞ √âtape 5 : Configurer l'automatisation

### 5.1 Cr√©er les jobs Cloud Scheduler
```bash
cd gcp_deployment/scripts

# Modifier le script pour votre projet
sed -i '' 's/similarweb-intel-dev/similarweb-data-[votre-nom]/g' setup_scheduler_jobs.sh

# Cr√©er les jobs
./setup_scheduler_jobs.sh
```

### 5.2 Jobs cr√©√©s

1. **similarweb-daily-extraction** (2h00 tous les jours)
   - Extrait les donn√©es de J-1

2. **auto-fill-missing** (3h00 le mercredi)
   - V√©rifie et remplit les donn√©es manquantes

3. **weekly-data-check** (9h00 le lundi)
   - Rapport hebdomadaire de compl√©tude

4. **monthly-report** (10h00 le 1er du mois)
   - Rapport mensuel d√©taill√©

### 5.3 V√©rifier les jobs
```bash
# Lister tous les jobs
gcloud scheduler jobs list --location=europe-west1

# D√©clencher manuellement un job
gcloud scheduler jobs run similarweb-daily-extraction \
  --location=europe-west1
```

## üìä √âtape 6 : Requ√™tes BigQuery utiles

### V√©rifier les derni√®res donn√©es
```sql
-- Derni√®res extractions de segments
SELECT 
  DATE(extraction_timestamp) as date,
  COUNT(DISTINCT segment_id) as nb_segments,
  MAX(extraction_timestamp) as derniere_extraction
FROM `similarweb-data-[votre-nom].similarweb_data.segments_data`
WHERE DATE(extraction_timestamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
GROUP BY date
ORDER BY date DESC;
```

### Analyser un segment sp√©cifique
```sql
-- Evolution d'un segment
SELECT 
  segment_name,
  date,
  visits,
  share,
  bounce_rate
FROM `similarweb-data-[votre-nom].similarweb_data.segments_data`
WHERE segment_name LIKE '%Leclerc%'
ORDER BY date DESC
LIMIT 30;
```

## üîç √âtape 7 : Monitoring et alertes

### 7.1 Configurer les m√©triques
```bash
# Cr√©er une alerte si la fonction √©choue
gcloud alpha monitoring policies create \
  --notification-channels=[YOUR_CHANNEL_ID] \
  --display-name="SimilarWeb Extraction Failed" \
  --condition-display-name="Function Error Rate > 0" \
  --condition-metric-type="cloudfunctions.googleapis.com/function/error_count"
```

### 7.2 Dashboard de monitoring
1. Aller dans la Console GCP > Monitoring
2. Cr√©er un nouveau dashboard
3. Ajouter les m√©triques :
   - Ex√©cutions de Cloud Functions
   - Erreurs de Cloud Functions
   - Requ√™tes BigQuery
   - Stockage utilis√©

## üö® D√©pannage

### Probl√®me : "Permission denied"
```bash
# Donner les permissions n√©cessaires
gcloud projects add-iam-policy-binding similarweb-data-[votre-nom] \
  --member="serviceAccount:similarweb-data-[votre-nom]@appspot.gserviceaccount.com" \
  --role="roles/bigquery.dataEditor"
```

### Probl√®me : "Quota exceeded"
- V√©rifier dans Console > IAM & Admin > Quotas
- Augmenter les quotas si n√©cessaire

### Probl√®me : Donn√©es manquantes
```bash
# V√©rifier l'√©tat d'une extraction sp√©cifique
bq query --use_legacy_sql=false '
SELECT * 
FROM `similarweb-data-[votre-nom].similarweb_data.segments_data`
WHERE DATE(extraction_timestamp) = "2025-01-15"
LIMIT 10'
```

## üí∞ Estimation des co√ªts

- **Cloud Functions** : ~0.50‚Ç¨/mois (400 ex√©cutions)
- **Cloud Scheduler** : Gratuit (< 3 jobs)
- **BigQuery Storage** : ~1‚Ç¨/mois (< 10 GB)
- **BigQuery Queries** : ~2‚Ç¨/mois
- **Total estim√©** : < 5‚Ç¨/mois

## ‚úÖ Checklist finale

- [ ] Projet GCP cr√©√© et configur√©
- [ ] APIs activ√©es
- [ ] BigQuery dataset et tables cr√©√©s
- [ ] Cloud Function d√©ploy√©e
- [ ] Cloud Scheduler jobs actifs
- [ ] Test d'extraction r√©ussi
- [ ] Donn√©es visibles dans BigQuery
- [ ] Monitoring configur√©

## üéâ F√©licitations !

Votre pipeline est maintenant op√©rationnel. Les donn√©es seront extraites automatiquement chaque jour √† 2h du matin.

### Prochaines √©tapes sugg√©r√©es
1. Connecter un outil de BI (Looker, Tableau, etc.)
2. Cr√©er des alertes sur les variations importantes
3. D√©velopper des mod√®les pr√©dictifs
4. Automatiser les rapports

Pour toute question, consultez la documentation dans le dossier `docs/`. 