"""
Script de backfill historique pour r√©cup√©rer toutes les donn√©es 2024 et 2025
Optimis√© pour minimiser les appels API et maximiser l'efficacit√©
"""
import sys
import os
from datetime import datetime
import time
import logging
from typing import List, Dict, Tuple
import argparse

# Ajouter le chemin parent pour importer les modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.config import *
from scripts.similarweb_api import SimilarWebAPI, save_results_to_json
from scripts.daily_extraction import extract_and_save_segments, extract_and_save_websites

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def get_historical_periods() -> List[Dict[str, str]]:
    """
    G√©n√®re la liste des p√©riodes √† extraire pour 2024 et 2025
    
    Returns:
        Liste des p√©riodes avec start_date et end_date
    """
    periods = []
    
    # Ann√©e 2024 compl√®te
    for month in range(1, 13):
        period = {
            'start_date': f'2024-{month:02d}',
            'end_date': f'2024-{month:02d}'
        }
        periods.append(period)
    
    # Ann√©e 2025 jusqu'√† mai
    current_date = datetime.now()
    max_month = min(current_date.month - 1, 5)  # Jusqu'√† mai ou le mois pr√©c√©dent
    
    for month in range(1, max_month + 1):
        period = {
            'start_date': f'2025-{month:02d}',
            'end_date': f'2025-{month:02d}'
        }
        periods.append(period)
    
    return periods


def estimate_api_calls(periods: List[Dict], segments_count: int, websites_count: int) -> Dict:
    """
    Estime le nombre d'appels API n√©cessaires
    
    Args:
        periods: Liste des p√©riodes
        segments_count: Nombre de segments
        websites_count: Nombre de sites web
        
    Returns:
        Estimation des appels API
    """
    # 3 appels par segment (avec les nouvelles m√©triques)
    segment_calls = len(periods) * segments_count * 3
    
    # 6 appels par site web
    website_calls = len(periods) * websites_count * 6
    
    total_calls = segment_calls + website_calls
    
    # Temps estim√© (1 seconde par appel + marges)
    estimated_time_minutes = (total_calls * 1.2) / 60
    
    return {
        'periods': len(periods),
        'segment_calls': segment_calls,
        'website_calls': website_calls,
        'total_calls': total_calls,
        'estimated_time_minutes': round(estimated_time_minutes, 1)
    }


def check_existing_data(period: Dict[str, str]) -> Tuple[bool, bool]:
    """
    V√©rifie si les donn√©es existent d√©j√† dans BigQuery
    
    Args:
        period: P√©riode √† v√©rifier
        
    Returns:
        Tuple (segments_exist, websites_exist)
    """
    try:
        from google.cloud import bigquery
        client = bigquery.Client(project=GCP_PROJECT_ID)
        
        # V√©rifier les segments
        query_segments = f"""
        SELECT COUNT(*) as count
        FROM `{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.segments_data`
        WHERE date = '{period['start_date']}-01'
        """
        
        result_segments = client.query(query_segments).result()
        segments_count = list(result_segments)[0].count
        
        # V√©rifier les sites web
        query_websites = f"""
        SELECT COUNT(*) as count
        FROM `{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.websites_data`
        WHERE date = '{period['start_date']}-01'
        """
        
        result_websites = client.query(query_websites).result()
        websites_count = list(result_websites)[0].count
        
        # On consid√®re que les donn√©es existent si on a au moins quelques entr√©es
        return segments_count > 10, websites_count > 0
        
    except Exception as e:
        logger.warning(f"Impossible de v√©rifier les donn√©es existantes: {e}")
        return False, False


def run_backfill(start_year: int = 2024, end_month: str = None, 
                 skip_existing: bool = True, limit_segments: int = None,
                 batch_size: int = 3):
    """
    Ex√©cute le backfill historique
    
    Args:
        start_year: Ann√©e de d√©but (2024 par d√©faut)
        end_month: Mois de fin au format YYYY-MM (automatique si None)
        skip_existing: Ignorer les p√©riodes d√©j√† pr√©sentes dans BigQuery
        limit_segments: Limiter le nombre de segments (None = tous)
        batch_size: Nombre de mois √† traiter par batch
    """
    logger.info("üöÄ D√©marrage du backfill historique")
    
    # Initialiser le client API
    api_client = SimilarWebAPI()
    
    # R√©cup√©rer le nombre de segments
    segments = api_client.get_custom_segments()
    if not segments:
        logger.error("Impossible de r√©cup√©rer les segments")
        return
    
    segments_count = len(segments) if not limit_segments else min(limit_segments, len(segments))
    
    # Charger la liste dynamique des sites web
    try:
        from scripts.manage_websites import load_websites
        domains = load_websites()
        websites_count = len(domains)
        logger.info(f"üìã {websites_count} sites web charg√©s depuis la configuration")
    except:
        websites_count = len(TARGET_DOMAINS)
        logger.warning(f"‚ö†Ô∏è  Utilisation de la liste par d√©faut: {websites_count} sites")
    
    # G√©n√©rer les p√©riodes
    periods = get_historical_periods()
    
    # Filtrer selon les param√®tres
    if start_year == 2025:
        periods = [p for p in periods if p['start_date'].startswith('2025')]
    
    if end_month:
        periods = [p for p in periods if p['start_date'] <= end_month]
    
    # Estimation
    estimation = estimate_api_calls(periods, segments_count, websites_count)
    
    logger.info(f"üìä Estimation du backfill:")
    logger.info(f"   - P√©riodes: {estimation['periods']} mois")
    logger.info(f"   - Segments: {segments_count}")
    logger.info(f"   - Sites web: {websites_count}")
    logger.info(f"   - Appels API totaux: {estimation['total_calls']:,}")
    logger.info(f"   - Temps estim√©: {estimation['estimated_time_minutes']} minutes")
    
    # Confirmation
    response = input("\n‚ö†Ô∏è  Voulez-vous continuer? (y/n): ")
    if response.lower() != 'y':
        logger.info("Backfill annul√©")
        return
    
    # Statistiques globales
    stats = {
        'periods_processed': 0,
        'periods_skipped': 0,
        'segments_extracted': 0,
        'websites_extracted': 0,
        'errors': 0
    }
    
    # Traiter par batch
    for i in range(0, len(periods), batch_size):
        batch = periods[i:i + batch_size]
        logger.info(f"\nüì¶ Traitement du batch {i//batch_size + 1}/{(len(periods) + batch_size - 1)//batch_size}")
        
        for period in batch:
            try:
                logger.info(f"\nüóìÔ∏è  P√©riode: {period['start_date']}")
                
                # V√©rifier si les donn√©es existent d√©j√†
                if skip_existing:
                    segments_exist, websites_exist = check_existing_data(period)
                    if segments_exist and websites_exist:
                        logger.info(f"   ‚è≠Ô∏è  Donn√©es d√©j√† pr√©sentes, passage au mois suivant")
                        stats['periods_skipped'] += 1
                        continue
                
                # Extraction des segments
                if not skip_existing or not segments_exist:
                    segment_stats = extract_and_save_segments(
                        api_client=api_client,
                        period=period,
                        limit=limit_segments
                    )
                    stats['segments_extracted'] += segment_stats['success']
                
                # Extraction des sites web
                if not skip_existing or not websites_exist:
                    website_stats = extract_and_save_websites(
                        api_client=api_client,
                        period=period
                    )
                    stats['websites_extracted'] += website_stats['success']
                
                stats['periods_processed'] += 1
                
                # Pause entre les p√©riodes
                logger.info(f"   ‚è∏Ô∏è  Pause de 5 secondes...")
                time.sleep(5)
                
            except Exception as e:
                logger.error(f"‚ùå Erreur pour la p√©riode {period['start_date']}: {e}")
                stats['errors'] += 1
                continue
        
        # Pause plus longue entre les batchs
        if i + batch_size < len(periods):
            logger.info(f"\n‚è∏Ô∏è  Pause de 30 secondes entre les batchs...")
            time.sleep(30)
    
    # R√©sum√© final
    logger.info("\n" + "="*50)
    logger.info("‚úÖ BACKFILL TERMIN√â")
    logger.info(f"   - P√©riodes trait√©es: {stats['periods_processed']}")
    logger.info(f"   - P√©riodes ignor√©es: {stats['periods_skipped']}")
    logger.info(f"   - Segments extraits: {stats['segments_extracted']}")
    logger.info(f"   - Sites web extraits: {stats['websites_extracted']}")
    logger.info(f"   - Erreurs: {stats['errors']}")
    
    return stats


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Backfill historique des donn√©es SimilarWeb')
    parser.add_argument('--year', type=int, default=2024, 
                        help='Ann√©e de d√©but (2024 ou 2025)')
    parser.add_argument('--end-month', type=str, 
                        help='Mois de fin au format YYYY-MM')
    parser.add_argument('--no-skip-existing', action='store_true',
                        help='Ne pas ignorer les donn√©es existantes')
    parser.add_argument('--limit-segments', type=int, 
                        help='Limiter le nombre de segments')
    parser.add_argument('--batch-size', type=int, default=3,
                        help='Nombre de mois par batch')
    
    args = parser.parse_args()
    
    run_backfill(
        start_year=args.year,
        end_month=args.end_month,
        skip_existing=not args.no_skip_existing,
        limit_segments=args.limit_segments,
        batch_size=args.batch_size
    ) 