"""
Script de backfill historique pour r√©cup√©rer toutes les donn√©es 2024 et 2025
VERSION CORRIG√âE - Compatible avec daily_extraction.py actuel
"""
import sys
import os
from datetime import datetime
import time
import logging
from typing import List, Dict, Tuple
import argparse

# Ajouter le chemin parent pour importer les modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.config import *
from scripts.similarweb_api import SimilarWebAPI, save_results_to_json

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def get_historical_periods() -> List[Dict[str, str]]:
    """
    G√©n√®re la liste des p√©riodes √† extraire pour 2024 et 2025
    
    Returns:
        Liste des p√©riodes avec start_date et end_date
    """
    periods = []
    
    # Ann√©e 2024 compl√®te
    for month in range(1, 13):
        period = {
            'start_date': f'2024-{month:02d}-01',
            'end_date': f'2024-{month:02d}-31'
        }
        periods.append(period)
    
    # Ann√©e 2025 jusqu'√† ao√ªt (mois actuel)
    current_date = datetime.now()
    max_month = min(current_date.month, 8)  # Jusqu'au mois actuel max
    
    for month in range(1, max_month + 1):
        # Calculer le dernier jour du mois
        if month == 2:
            last_day = 28  # F√©vrier (simplifi√©)
        elif month in [4, 6, 9, 11]:
            last_day = 30
        else:
            last_day = 31
            
        period = {
            'start_date': f'2025-{month:02d}-01',
            'end_date': f'2025-{month:02d}-{last_day:02d}'
        }
        periods.append(period)
    
    return periods


def extract_and_save_segments(api_client: SimilarWebAPI, period: Dict[str, str], 
                             limit: int = None) -> Dict:
    """
    Extrait et sauvegarde les segments pour une p√©riode donn√©e
    
    Args:
        api_client: Instance du client API
        period: Dictionnaire avec start_date et end_date
        limit: Limite du nombre de segments
        
    Returns:
        Statistiques de l'extraction
    """
    logger.info(f"üìä Extraction segments pour {period['start_date'][:7]}")
    
    # Convertir les dates au format YYYY-MM pour l'API
    start_month = period['start_date'][:7]  # YYYY-MM
    end_month = period['end_date'][:7]      # YYYY-MM
    
    # Extraction des segments
    segments_data = api_client.extract_all_segments(
        start_date=start_month,
        end_date=end_month,
        limit=limit,
        user_only=True  # Utiliser seulement les segments utilisateur
    )
    
    # Statistiques
    stats = {
        'total': len(segments_data),
        'success': len([s for s in segments_data if not s.get('error')]),
        'errors': len([s for s in segments_data if s.get('error')])
    }
    
    # Sauvegarde avec timestamp
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"segments_extraction_{start_month.replace('-', '')}_{timestamp}.json"
    save_results_to_json(segments_data, filename)
    
    logger.info(f"‚úÖ Segments {period['start_date'][:7]}: {stats['success']}/{stats['total']} extraits")
    
    return stats


def extract_and_save_websites(api_client: SimilarWebAPI, period: Dict[str, str]) -> Dict:
    """
    Extrait et sauvegarde les sites web pour une p√©riode donn√©e
    
    Args:
        api_client: Instance du client API  
        period: Dictionnaire avec start_date et end_date
        
    Returns:
        Statistiques de l'extraction
    """
    logger.info(f"Extraction websites pour {period['start_date'][:7]}")
    
    # Convertir les dates au format YYYY-MM pour l'API
    start_month = period['start_date'][:7]  # YYYY-MM
    end_month = period['end_date'][:7]      # YYYY-MM
    
    # Charger la liste des sites web
    try:
        from scripts.manage_websites import load_websites
        domains = load_websites()
        logger.info(f"üìã {len(domains)} sites web charg√©s")
    except:
        domains = TARGET_DOMAINS
        logger.warning(f"Utilisation de la liste par d√©faut: {len(domains)} sites")
    
    # Extraction des sites web
    websites_data = api_client.extract_all_websites(
        domains=domains,
        start_date=start_month,
        end_date=end_month
    )
    
    # Statistiques
    stats = {
        'total': len(websites_data),
        'success': len([w for w in websites_data if any(w.get('metrics', {}).values())]),
        'errors': len([w for w in websites_data if not any(w.get('metrics', {}).values())])
    }
    
    # Sauvegarde avec timestamp
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"websites_extraction_{start_month.replace('-', '')}_{timestamp}.json"
    save_results_to_json(websites_data, filename)
    
    logger.info(f"‚úÖ Websites {period['start_date'][:7]}: {stats['success']}/{stats['total']} extraits")
    
    return stats


def estimate_api_calls(periods: List[Dict], segments_count: int, websites_count: int) -> Dict:
    """
    Estime le nombre d'appels API n√©cessaires
    """
    # 3 appels par segment (avec les nouvelles m√©triques)
    segment_calls = len(periods) * segments_count * 3
    
    # 6 appels par site web
    website_calls = len(periods) * websites_count * 6
    
    total_calls = segment_calls + website_calls
    
    # Temps estim√© (1.5 seconde par appel + marges)
    estimated_time_minutes = (total_calls * 1.5) / 60
    
    return {
        'periods': len(periods),
        'segment_calls': segment_calls,
        'website_calls': website_calls,
        'total_calls': total_calls,
        'estimated_time_minutes': round(estimated_time_minutes, 1)
    }


def run_backfill(start_year: int = 2024, end_month: str = None, 
                 limit_segments: int = None, batch_size: int = 3):
    """
    Ex√©cute le backfill historique
    
    Args:
        start_year: Ann√©e de d√©but (2024 par d√©faut)
        end_month: Mois de fin au format YYYY-MM (automatique si None)
        limit_segments: Limiter le nombre de segments (None = tous)
        batch_size: Nombre de mois √† traiter par batch
    """
    logger.info("üöÄ D√©marrage du backfill historique")
    
    # Initialiser le client API
    api_client = SimilarWebAPI()
    
    # R√©cup√©rer le nombre de segments
    segments = api_client.get_custom_segments(user_only=True)
    if not segments:
        logger.error("Impossible de r√©cup√©rer les segments")
        return
    
    segments_count = len(segments) if not limit_segments else min(limit_segments, len(segments))
    
    # Charger la liste dynamique des sites web
    try:
        from scripts.manage_websites import load_websites
        domains = load_websites()
        websites_count = len(domains)
        logger.info(f"üìã {websites_count} sites web charg√©s depuis la configuration")
    except:
        websites_count = len(TARGET_DOMAINS)
        logger.warning(f"‚ö†Ô∏è Utilisation de la liste par d√©faut: {websites_count} sites")
    
    # G√©n√©rer les p√©riodes
    periods = get_historical_periods()
    
    # Filtrer selon les param√®tres
    if start_year == 2025:
        periods = [p for p in periods if p['start_date'].startswith('2025')]
    elif start_year == 2024:
        periods = [p for p in periods if p['start_date'].startswith('2024')]
    
    if end_month:
        periods = [p for p in periods if p['start_date'][:7] <= end_month]
    
    # Estimation
    estimation = estimate_api_calls(periods, segments_count, websites_count)
    
    logger.info(f"üìä ESTIMATION DU BACKFILL:")
    logger.info(f"   - P√©riodes: {estimation['periods']} mois")
    logger.info(f"   - Segments: {segments_count}")
    logger.info(f"   - Sites web: {websites_count}")
    logger.info(f"   - Appels API totaux: {estimation['total_calls']:,}")
    logger.info(f"   - Temps estim√©: {estimation['estimated_time_minutes']} minutes")
    
    logger.info(f"\nüìÖ P√©riodes √† extraire:")
    for period in periods:
        logger.info(f"   - {period['start_date'][:7]}")
    
    # Confirmation
    response = input(f"\n‚ö†Ô∏è Voulez-vous continuer avec {len(periods)} mois? (y/n): ")
    if response.lower() != 'y':
        logger.info("Backfill annul√©")
        return
    
    # Statistiques globales
    stats = {
        'periods_processed': 0,
        'segments_extracted': 0,
        'websites_extracted': 0,
        'errors': 0,
        'start_time': datetime.now()
    }
    
    # Traiter par batch
    for i in range(0, len(periods), batch_size):
        batch = periods[i:i + batch_size]
        logger.info(f"\nüì¶ BATCH {i//batch_size + 1}/{(len(periods) + batch_size - 1)//batch_size}")
        
        for period in batch:
            try:
                logger.info(f"\nüóìÔ∏è P√©riode: {period['start_date'][:7]}")
                
                # Extraction des segments
                segment_stats = extract_and_save_segments(
                    api_client=api_client,
                    period=period,
                    limit=limit_segments
                )
                stats['segments_extracted'] += segment_stats['success']
                
                # Extraction des sites web
                website_stats = extract_and_save_websites(
                    api_client=api_client,
                    period=period
                )
                stats['websites_extracted'] += website_stats['success']
                
                stats['periods_processed'] += 1
                
                # Pause entre les p√©riodes
                logger.info(f"   ‚è∏Ô∏è Pause de 5 secondes...")
                time.sleep(5)
                
            except Exception as e:
                logger.error(f"‚ùå Erreur pour la p√©riode {period['start_date'][:7]}: {e}")
                stats['errors'] += 1
                continue
        
        # Pause plus longue entre les batchs
        if i + batch_size < len(periods):
            logger.info(f"\n‚è∏Ô∏è Pause de 30 secondes entre les batchs...")
            time.sleep(30)
    
    # R√©sum√© final
    duration = (datetime.now() - stats['start_time']).total_seconds() / 60
    
    logger.info("\n" + "="*50)
    logger.info("‚úÖ BACKFILL TERMIN√â")
    logger.info(f"   - Dur√©e: {duration:.1f} minutes")
    logger.info(f"   - P√©riodes trait√©es: {stats['periods_processed']}")
    logger.info(f"   - Segments extraits: {stats['segments_extracted']}")
    logger.info(f"   - Sites web extraits: {stats['websites_extracted']}")
    logger.info(f"   - Erreurs: {stats['errors']}")
    
    logger.info(f"\nüìÅ Fichiers cr√©√©s dans le dossier 'data/'")
    logger.info(f"   - Utilisez: python scripts/upload_to_bigquery.py --type all")
    
    return stats


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Backfill historique des donn√©es SimilarWeb')
    parser.add_argument('--year', type=int, default=2024, 
                        help='Ann√©e de d√©but (2024 ou 2025)')
    parser.add_argument('--end-month', type=str, 
                        help='Mois de fin au format YYYY-MM')
    parser.add_argument('--limit-segments', type=int, 
                        help='Limiter le nombre de segments')
    parser.add_argument('--batch-size', type=int, default=3,
                        help='Nombre de mois par batch')
    
    args = parser.parse_args()
    
    run_backfill(
        start_year=args.year,
        end_month=args.end_month,
        limit_segments=args.limit_segments,
        batch_size=args.batch_size
    )