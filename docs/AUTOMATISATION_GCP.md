# ğŸ¤– Automatisation GCP - Cloud Scheduler

## ğŸ“‹ Vue d'ensemble

Le projet utilise Google Cloud Platform pour automatiser l'extraction quotidienne des donnÃ©es SimilarWeb. L'automatisation est composÃ©e de 4 jobs Cloud Scheduler qui dÃ©clenchent des Cloud Functions.

## ğŸ—ï¸ Architecture

```mermaid
graph TD
    A[Cloud Scheduler] -->|"Tous les jours Ã  2h00"| B[similarweb-daily-extraction]
    A -->|"Mercredis Ã  3h00"| C[auto-fill-missing]
    A -->|"Lundis Ã  9h00"| D[weekly-data-check]
    A -->|"1er du mois Ã  10h00"| E[monthly-report]
    
    B --> F[Cloud Function<br/>similarweb-daily-extraction]
    C --> G[Cloud Function<br/>similarweb-data-checker]
    D --> G
    E --> G
    
    F --> H[API SimilarWeb]
    H --> I[Extraction des donnÃ©es]
    I --> J[BigQuery<br/>segments_data]
    I --> K[BigQuery<br/>websites_data]
```

## ğŸ“… Jobs Cloud Scheduler

### 1. **similarweb-daily-extraction** (Quotidien - 2h00)
- **Fonction** : Extraction des donnÃ©es J-1
- **FrÃ©quence** : Tous les jours Ã  2h00 (heure de Paris)
- **DurÃ©e** : ~1-2 minutes
- **DonnÃ©es extraites** :
  - 159 segments personnalisÃ©s
  - 21 sites web
- **Destination** : BigQuery `similarweb_data.segments_data` et `similarweb_data.websites_data`

### 2. **auto-fill-missing** (Hebdomadaire - Mercredi 3h00)
- **Fonction** : Rattrapage automatique des donnÃ©es manquantes
- **StratÃ©gie** : VÃ©rifie les 7 derniers jours
- **Actions** :
  - DÃ©tecte les trous de donnÃ©es
  - Lance une extraction rÃ©troactive
  - Envoie un rapport par email

### 3. **weekly-data-check** (Hebdomadaire - Lundi 9h00)
- **Fonction** : Rapport de complÃ©tude hebdomadaire
- **Contenu** :
  - Taux de complÃ©tude par segment
  - DonnÃ©es manquantes identifiÃ©es
  - Recommandations d'actions

### 4. **monthly-report** (Mensuel - 1er du mois 10h00)
- **Fonction** : Rapport mensuel dÃ©taillÃ©
- **Analyses** :
  - Tendances du trafic
  - Comparaisons YoY
  - Top performers/dÃ©clineurs

## ğŸ› ï¸ Configuration

### Variables d'environnement
```bash
SIMILARWEB_API_KEY=<votre_clÃ©_api>
GCP_PROJECT_ID=similarweb-intel-dev
USER_ONLY_SEGMENTS=true
```

### SchÃ©ma BigQuery

#### Table: segments_data
```sql
- segment_id: STRING
- segment_name: STRING
- date: DATE
- visits: INTEGER
- share: FLOAT
- bounce_rate: FLOAT
- pages_per_visit: FLOAT
- visit_duration: FLOAT
- page_views: INTEGER
- unique_visitors: INTEGER
- extraction_timestamp: TIMESTAMP
```

#### Table: websites_data
```sql
- domain: STRING
- date: DATE
- visits: INTEGER
- bounce_rate: FLOAT
- pages_per_visit: FLOAT
- visit_duration: FLOAT
- page_views: INTEGER
- unique_visitors: INTEGER
- desktop_share: FLOAT
- mobile_share: FLOAT
- extraction_timestamp: TIMESTAMP
```

## ğŸš€ Commandes utiles

### VÃ©rifier l'Ã©tat des jobs
```bash
gcloud scheduler jobs list --location=europe-west1
```

### Voir les logs
```bash
# Logs de la derniÃ¨re extraction
gcloud functions logs read similarweb-daily-extraction --limit=50 --region=europe-west1

# Logs avec filtre temporel
gcloud functions logs read similarweb-daily-extraction \
  --region=europe-west1 \
  --start-time="2025-06-20T00:00:00Z" \
  --end-time="2025-06-21T00:00:00Z"
```

### DÃ©clencher manuellement
```bash
# Extraction quotidienne
gcloud scheduler jobs run similarweb-daily-extraction --location=europe-west1

# VÃ©rification hebdomadaire
gcloud scheduler jobs run weekly-data-check --location=europe-west1
```

### Modifier les horaires
```bash
# Changer l'heure d'extraction (exemple: 3h au lieu de 2h)
gcloud scheduler jobs update http similarweb-daily-extraction \
  --location=europe-west1 \
  --schedule="0 3 * * *"
```

## ğŸ”§ Maintenance

### RedÃ©ployer la Cloud Function
```bash
cd gcp_deployment/scripts
./deploy_cloud_function.sh
```

### Mettre Ã  jour la configuration
1. Modifier `gcp_deployment/cloud_functions/config.py`
2. RedÃ©ployer la fonction
3. Tester avec un dÃ©clenchement manuel

### Ajouter un nouveau site
1. Ajouter dans `config.py` local
2. Copier vers `gcp_deployment/cloud_functions/config.py`
3. RedÃ©ployer

## ğŸ“Š Monitoring

### Dashboard BigQuery
```sql
-- VÃ©rifier les derniÃ¨res extractions
SELECT 
  DATE(extraction_timestamp) as date,
  COUNT(DISTINCT segment_id) as segments_extracted,
  MAX(extraction_timestamp) as last_extraction
FROM `similarweb-intel-dev.similarweb_data.segments_data`
WHERE DATE(extraction_timestamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
GROUP BY date
ORDER BY date DESC;
```

### Alertes configurÃ©es
- âŒ Ã‰chec d'extraction â†’ Email immÃ©diat
- âš ï¸ DonnÃ©es manquantes > 2 jours â†’ Alerte quotidienne
- ğŸ“Š Rapport hebdomadaire â†’ Tous les lundis

## ğŸš¨ Troubleshooting

### Erreur "Permission denied"
```bash
# VÃ©rifier les permissions
gcloud projects get-iam-policy similarweb-intel-dev

# Ajouter les permissions nÃ©cessaires
gcloud projects add-iam-policy-binding similarweb-intel-dev \
  --member="user:votre-email@gmail.com" \
  --role="roles/cloudfunctions.admin"
```

### Erreur "API quota exceeded"
- VÃ©rifier le quota SimilarWeb (10k calls/mois)
- RÃ©duire temporairement le nombre de segments
- Attendre le reset mensuel

### DonnÃ©es manquantes
1. VÃ©rifier les logs de la fonction
2. Lancer `auto-fill-missing` manuellement
3. Si Ã©chec, utiliser le script local de backfill

## ğŸ“ˆ MÃ©triques de performance

- **Temps d'extraction moyen** : 90-120 secondes
- **Taux de succÃ¨s** : 98%+
- **CoÃ»t mensuel estimÃ©** : < 5â‚¬
- **API calls par jour** : ~320 (segments) + ~63 (websites) 